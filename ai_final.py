# -*- coding: utf-8 -*-
"""AI_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ptrJwR5qut9jqEHfqwwIMQ_E0v0QrGlo
"""

#installing dependencies
!pip install openai==0.28
!pip install pycountry
!pip install textstat

#Importing libraries and dependencies
import pandas as pd
import numpy as np
import nltk, spacy, string, re, os, openai
import matplotlib.pyplot as plt
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import plotly.express as px
import plotly.graph_objects as go
nltk.download('wordnet')
nltk.download('punkt')
from sklearn.pipeline import Pipeline
import pycountry
from nltk.util import ngrams
import seaborn as sns
import geopandas as gpd

"""##Loading the data"""

from google.colab import drive, files
drive.mount('/content/drive')

data_1 = pd.read_csv('/content/drive/My Drive/AI_2023/final_project/0401_UkraineCombinedTweetsDeduped.csv')
data_2 =  pd.read_csv('/content/drive/My Drive/AI_2023/final_project/0403_UkraineCombinedTweetsDeduped.csv')

#Merging the 2 datasets
data = pd.concat([data_1, data_2], axis=0, ignore_index=True)

"""##EDA1: Understanding the dataset and selecting key features)"""

data.describe

data.info()

#Selecting key features for the dataset
final_data = data[['username', 'location', 'text', 'language', 'tweetcreatedts','followers']]

final_data.head()

final_data.isnull().sum()

#Removing all rows with null values to remove all rows with no locations
final_data.dropna(inplace=True)

final_data.info()

#splitting loccation into country and region
split_columns = final_data["location"].str.split(',', n=1, expand=True)
final_data['country'] = split_columns[0]
final_data['region'] = split_columns[1]
final_data.drop(columns=["location"], inplace=True)

#Checking for the countries with the most tweets after separating countries and regions
final_data['country'].value_counts(normalize=True)

#checking if the country exist
def is_valid_country(country_name):
    try:
        pycountry.countries.lookup(country_name)
        return True
    except LookupError:
        return False
final_data = final_data[final_data['country'].apply(is_valid_country)]

#Extracting date and time from timestapm format
final_data['datetime_column'] = pd.to_datetime(final_data['tweetcreatedts'], format='%Y-%m-%d %H:%M:%S.%f')

final_data['date'] = final_data['datetime_column'].dt.date
final_data['time'] = final_data['datetime_column'].dt.time

# Drop the original datetime column if needed
final_data.drop(columns=['tweetcreatedts', 'datetime_column'], inplace=True)

final_data['country'].value_counts

#Plotting the distribution of languages for the tweets dataset
value_counts = final_data['country'].value_counts(normalize=True)
# Apply the threshold to select only the best locations
threshold = 0.001
selected_locations = value_counts[value_counts > threshold]

# Create a custom template for the plot
custom_template = go.layout.Template(
    layout=dict(
        plot_bgcolor='white',
    )
)

# Create the bar chart using Plotly Express
fig = px.bar(selected_locations,
             title='Distribution of tweets based on the countries',
             labels={'value': 'Location Percentage', 'index': 'Location'},
             color_discrete_sequence=['blue'],
             opacity=0.6,
             template=custom_template,
             )

# Adjust the layout
fig.update_layout(
    bargap=0.6
)

# Show the plot
fig.show()

#top 10 countriesin terms of informative tweets linked to the crisis in Ukraine
df_country = pd.DataFrame(final_data[final_data['country'] != 'Unknown']['country'].value_counts()).reset_index()
df_country.columns = ['Country', 'Count']

# Calculate unique country count for each row
final_data['unique_country_count'] = final_data.groupby('country')['country'].transform('nunique')

# Display the top 10 rows
df_country['Percentage'] = round(df_country['Count'] / final_data['unique_country_count'].sum() * 100, 1)
df_country = df_country[['Country', 'Count', 'Percentage']].head(10)
print(df_country)

# Load the world map shapefile
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
# Merge the country data onto the GeoDataFrame
world = world.merge(df_country, left_on='name', right_on='Country', how='left')
#plot
fig, ax = plt.subplots(1,2, figsize=(15,6))
plt.subplot(1,2,1)
ax=world.plot(column='Percentage', cmap='PuBuGn', legend=True,
              legend_kwds={'label': "Percentage by Tweet Countries",'orientation':'horizontal'},
              figsize=(10,6),linewidth=1, edgecolor='0.5', ax=ax[0])
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
ax.set_title('Geographic Distribution of Twitter Users Discussing Ukraine War',size=14)

plt.subplot(1,2,2)
ax = sns.barplot(y=df_country['Country'][0:10], x=df_country['Count'][0:10], palette = 'PuBuGn')
ax.set_title('Top 10 Countries Discussing About War in Ukraine')
for p in ax.patches:
    ax.text(p.get_width(), p.get_y()+0.5, int(p.get_width()), va='center', size=12)
plt.tight_layout()
plt.show()

"""* Most tweets about the war in Ukraine are from European and North American countries. The top 5 countries discussing this topic are : Ukraine, United States,  France and Canada, India
* 14.0% of the tweets are from Ukraine.
"""

#Checking the most common language used for tweets
final_data['language'].value_counts(normalize=True)

#Plotting the distribution of languages for the tweets dataset
value_counts = final_data['language'].value_counts(normalize=True)
threshold = 0.001
selected_locations = value_counts[value_counts > threshold]
custom_template = go.layout.Template(
    layout=dict(
        plot_bgcolor='white',
    )
)
fig = px.bar(selected_locations,
         title = 'Distribution of tweets based on the Language spoken',
         labels={'value':'Langauage Percentage', 'index':'Languages'},
         color_discrete_sequence = ['blue'],
         opacity=0.6,
         template = custom_template,)
fig.update_layout(
    bargap=0.6
)
fig.show()

"""* 70+ of the tweets are in English
* The top five languages are English, French, Spanish, German and Russian
"""

#Plotting the most active commenter in the Ukraine war discussion
df_user = pd.DataFrame(data["username"].value_counts()).reset_index()
df_user = df_user.rename(columns={'index': 'User', 'username': 'Counts'})

bins = [0, 1, 10, 50, 100, df_user["Counts"].max()]
labels = ['1', '2-10', '10-50', '50-100', '>100']
df_user['Level'] = pd.cut(df_user['Counts'], bins=bins, labels=labels)

fig, ax = plt.subplots(1, 2, figsize=(12, 5))
plt.subplot(1, 2, 1)
ax = sns.barplot(x=df_user['Level'].value_counts().index, y=df_user['Level'].value_counts(), palette='PuBuGn')
ax.set_ylabel('Counts of Users')
ax.set_xlabel('Number of Tweets')
ax.set_title('User Activity in Discussions about War in Ukraine')
for p in ax.patches:
    ax.text(p.get_width() / 2 + p.get_x(), p.get_height(), int(p.get_height()), ha='center', size=12)
ax.set_yscale('log')

plt.subplot(1, 2, 2)
ax = sns.barplot(y=df_user['User'][0:10], x=df_user['Counts'][0:10], palette='PuBuGn')
for p in ax.patches:
    ax.text(p.get_width(), p.get_y() + 0.5, int(p.get_width()), va='center', size=10)
ax.set_title('Top 10 Most Active Ukraine war Discussing Users')
plt.tight_layout()
plt.show()

total = df_user.shape[0]
once = round(df_user[df_user['Level'] == '1'].shape[0] / total * 100, 2)
print(f'There are a total of {total} \
twitter users tweeted about the War in Ukraine. \n{once}% users have tweeted only once, while {100 - once}% users have tweeted more than once.')

##Plotting the most popular commenter in the Ukraine war discussion
fig,ax= plt.subplots(1,2,figsize=(15,6))
#followers distribution
plt.subplot(1,2,1)
bins = [0, 10, 100, 1000, 1e4, 1e5, 1e6,1e7,1e8]
ax = sns.histplot(data['followers'], bins=bins, color='#a6bddb')
ax.set_xscale('log')
ax.set_title("Distribution of Followers among Ukraine War-Twitter Users")
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()),ha='center',va='center',size=12)

#Top 10 Users
plt.subplot(1,2,2)
df_fo=data[['username', 'followers']]
df_fo=df_fo.sort_values('followers', ascending=False)
df_fo = df_fo.drop_duplicates(subset=['username'], keep='first')
ax = sns.barplot(y=df_fo['username'][0:10], x=df_fo['followers'][0:10], palette='PuBuGn')
ax.set_title('Top 10 Ukraine War-Twitter Users with the Most Followers')
for p in ax.patches:
    ax.annotate(int(p.get_width()), (p.get_width(), p.get_y()+0.5), size=12)
plt.tight_layout()
plt.show()

"""* Most users tweeting about the war in Ukraine have from 0- 5000 followers
* Some popular twitter users with over million followers also care about the war in Ukraine, such as (pontifex, Forbes, ndtv, aajtak)
"""

#List of most active users in the Ukraine war discussion
df_fo=data[['username', 'followers']]
df_fo=df_fo.sort_values('followers', ascending=False)
df_fo = df_fo.drop_duplicates(subset=['username'], keep='first')
df_fo[0:10]

"""Ngram Exploration"""

from sklearn.feature_extraction.text import CountVectorizer

def get_top_ngram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx])
                  for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:10]

top_n_bigrams=get_top_ngram(final_data['text'],2)[:20]
x,y=map(list,zip(*top_n_bigrams))
ax=sns.barplot(x=y,y=x)
ax.set_xlim(0, 10000)
plt.show()

"""* The most commming grouping of 2 words in the original text are: **http co, of the, in the, in Ukraine and Ukraine https**
* This shows a high number of hyperlinks in the original data plus the excessive use of english articles.
"""

top_n_bigrams=get_top_ngram(final_data['text'],3)[:20]
x,y=map(list,zip(*top_n_bigrams))
ax=sns.barplot(x=y,y=x)
ax.set_xlim(0, 5000)
plt.show()

"""* The most commming grouping of 3 words in the original text are: **ukraine http co, ukraine ukrainewar russia, and stoprussia https co**
* This shows a high number of hyperlinks in the original data plus the excessive use of english articles.
* the output shows a preliminary high support toward Ukraine from the users with most tweets being linked to "stop Russia" or "stand for Ukraine"
"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Join all processed tweets into a single string
all_tweets = ' '.join(final_data['text'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(all_tweets)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""* The wordcloud shows the words **https, t, Ukraine, co, UkraineWar, Russia, Putin** are highly used in the tweets.
* IT also shows a high concentration of links in the tweets. Thus, implying a preliminary cleaning may provide better results when passing the text into the model.

Summary of Exploratory Data Analysis (EDA) on War in Ukraine Tweets:

The exploratory data analysis (EDA) on tweets related to the war in Ukraine reveals several key insights:

    Geographic Distribution:
        The majority of tweets originate from European and North American countries.
        The top five countries actively discussing the war are Ukraine, United States, France, Canada, and India.
        Notably, 14.0% of the tweets are specifically from Ukraine.

    Language Diversity:
        Over 70% of the tweets are in English, making it the predominant language.
        The top five languages used in tweets are English, French, Spanish, German, and Russian.

    User Engagement:
        A substantial number of Twitter users, totaling 310,421, have tweeted about the war in Ukraine.
        Among these users, 64.02% have tweeted only once, while 35.98% have tweeted more than once.

    Follower Distribution:
        Most users tweeting about the war have followers ranging from 0 to 5000.
        Interestingly, some influential Twitter accounts with over a million followers, such as @pontifex, @Forbes, @ndtv, and @aajtak, are actively engaged in discussing the war.

    Common Word Groupings:
        The most common two-word groupings in tweets include "http co," "of the," "in the," "in Ukraine," and "Ukraine https."
        Similarly, the most common three-word groupings involve hyperlinks and phrases like "Ukraine http co," "Ukraine UkraineWar Russia," and "stop Russia https co."
        This suggests a prevalence of hyperlinks and frequent use of English articles in the tweets.

    Word Cloud Analysis:
        The word cloud highlights frequent use of words such as "https," "t," "Ukraine," "co," "UkraineWar," "Russia," and "Putin."
        The concentration of links in the tweets is evident in the word cloud.
        The analysis suggests that preliminary cleaning of the text data might enhance the quality of input when passing it into a model.

##Data Preprocessing
"""

# Define some common contractions in English
contractions = {
    "ain't": "am not",
    "aren't": "are not",
    "can't": "cannot",
    "couldn't": "could not",
    "didn't": "did not",
    "doesn't": "does not",
    "don't": "do not",
    "hadn't": "had not",
    "hasn't": "has not",
    "haven't": "have not",
    "he's": "he is",
    "I'd": "I would",
    "I'll": "I will",
    "I'm": "I am",
    "I've": "I have",
    "isn't": "is not",
    "it's": "it is",
    "let's": "let us",
    "mustn't": "must not",
    "shan't": "shall not",
    "she's": "she is",
    "shouldn't": "should not",
    "that's": "that is",
    "there's": "there is",
    "they're": "they are",
    "we're": "we are",
    "weren't": "were not",
    "what's": "what is",
    "won't": "will not",
    "wouldn't": "would not",
    "you'd": "you would",
    "you'll": "you will",
    "you're": "you are",
    "you've": "you have",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "won't": "will not",
    "wouldn't": "would not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't": "have not",
    "hadn't": "had not",
    "doesn't": "does not",
    "don't": "do not",
    "didn't": "did not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "hasn't": "has not",
    "haven't":"have not"
}

#Removing Punctuations
def preprocess_tweet(tweet):
  res_tweet = []
  lemmatizer = WordNetLemmatizer()
  for word in tweet.split():
    #Expand Contractions
    word = contractions.get(word.lower(), word)
    #Remove stopwords
    if word.lower() not in stopwords.words('english'):
      # Remove URLs
      word = re.sub(r'http\S+', '', word)

      # remove usernames
      word = re.sub('@[\w]+', '', word)

      # removing hashtags
      word = re.sub("([^0-9A-Za-z \t])|(\w+:\/\/\s+)", '', word)

      emoji_clean = re.compile("["
                        "\U0001F600-\U0001F64F"
                        "\U0001F300-\U0001F5FF"
                        "\U0001F680-\U0001F6FF"
                        "\U0001F1E0-\U0001F1FF"
                        "\U00002702-\U000027B0"
                        "\U000024C2-\U0001F251"
                        "]+", flags=re.UNICODE)
      word = emoji_clean.sub(r'', word)

      #remove punctations
      word = re.sub(r'[^\w\s]', '', word)
      #convert to lower case
      word = word.lower()

      #lemmatize the word
      word = lemmatizer.lemmatize(word, pos='v')

      if word != '':
        res_tweet.append(word)
  return ' '.join([word for word in res_tweet])

final_data['processed_text'] = final_data['text'].apply(preprocess_tweet)

# Display the first few rows of the DataFrame with the processed text
print(final_data[['text', 'processed_text']].head())

"""##EDA2


"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Join all processed tweets into a single string
all_tweets = ' '.join(final_data['processed_text'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(all_tweets)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""#Results from the wordcloud
The wordcloud shows that the words ***ukraine***,  ***russia***, ***stop russia***, ***air force*** and
 ***ukranian war*** are very pronounced in the text with the words featuring in most of the tweets. This is to be expected as the context of the tweets is based on the crisis currently going on in Ukraine. Besides, the output is free from the previous ketwords  linked to the extensive hypertext in the tweets.

**Ngram exploration after preprocessing**
"""

top_n_bigrams=get_top_ngram(final_data['processed_text'],2)[:20]
x,y=map(list,zip(*top_n_bigrams))
ax=sns.barplot(x=y,y=x)
ax.set_xlim(0, 2000)
plt.show()

"""* The most common pair of word in the tweets were **war crimes, russia ukraine, ukraine russia, russian army, ukraine ukraine war** which are highly linked to the war in Ukraine and thus will be helpful in understading the situatiuon on ground.
* After preprocessing, the output seem more close to English words without excessive articles compared ot the original text.  
"""

top_n_bigrams=get_top_ngram(final_data['processed_text'],3)[:20]
x,y=map(list,zip(*top_n_bigrams))
ax=sns.barplot(x=y,y=x)
ax.set_xlim(0, 1500)
plt.show()

"""* The most common pair of word in the tweets were **ukraine ukrainewar, russia, ukrainewar russia ukraineinvasion, standwithukraine stopthewar ukraine** which are highly linked to the war in Ukraine and thus will be helpful in understading the situatiuon on ground.
* After preprocessing, the output seem more close to English words without excessive articles compared ot the original text.  
* Besides, these outputs look more like mantras used by users to promote mass attention to the war atroctities.

**Summary of EDA Results:**

The exploratory data analysis (EDA) of the proprocessed text reveals significant patterns and insights related to the ongoing crisis in Ukraine. The wordcloud prominently features keywords such as "Ukraine," "Russia," "Stop Russia," "Air Force," and "Ukrainian War," indicating that these terms are prevalent across the tweets. This aligns with expectations, given the context of the tweets revolving around the crisis in Ukraine. Notably, the analysis indicates the absence of previous keywords, suggesting a shift in focus and language in recent tweets.

Furthermore, the most common word pairs extracted from the tweets, such as "war crimes," "Russia Ukraine," "Ukraine Russia," "Russian Army," and "Ukraine Ukraine War," provide valuable insights into the prevailing themes and topics discussed on the platform. These pairs are highly linked to the war in Ukraine and contribute to a deeper understanding of the situation on the ground.

After preprocessing, the output demonstrates a refinement in language, with the text appearing more aligned with standard English, and excessive articles removed. This enhancement contributes to a clearer and more comprehensible representation of the underlying content.

Additionally, another set of common word pairs, including "Ukraine UkraineWar," "Russia," "UkraineWar Russia UkraineInvasion," "StandWithUkraine," and "StopTheWar Ukraine," suggests a strategic use of language by users to draw mass attention to the war atrocities. These phrases appear to serve as impactful mantras aimed at promoting awareness and support for Ukraine during the crisis.

In conclusion, the EDA provides a comprehensive view of the key terms, prevalent word pairs, and linguistic transformations within the tweets, offering valuable insights into the ongoing crisis in Ukraine and the discourse surrounding it on the platform.

##Feature Importance
"""

from sklearn.model_selection import train_test_split
from nltk.classify import NaiveBayesClassifier
from nltk.util import ngrams
import nltk

# Function to extract bigrams from a text
def extract_bigrams(text):
    return list(ngrams(text.split(), 2))
new_dataframe = pd.DataFrame({'text': final_data['text'].head(10000)})
new_dataframe_2 = pd.DataFrame({'text': final_data['processed_text'].head(10000)})
# Combine the reviews into separate datasets
dataframes = [new_dataframe, new_dataframe_2]

for index, dataframe in enumerate(dataframes):
    reviews = dataframe['text'].head(10000)

    # Split the data into training and testing sets
    training_data, testing_data = train_test_split(reviews, test_size=0.2, random_state=42)

    # Extract bigrams from the reviews and create a feature set
    features = [(dict(extract_bigrams(review)), None) for review in training_data]

    # Train a Naive Bayes classifier on the feature set
    classifier = NaiveBayesClassifier.train(features)

    # Evaluate the classifier on the testing data
    accuracy = nltk.classify.accuracy(classifier, [(dict(extract_bigrams(review)), None) for review in testing_data])

    print(f"Accuracy for DataFrame {index + 1}: {accuracy}")

"""The processed and  originals tweets have the same accuracy in classfying the text into mock categories"""

!pip install textatistic

from textatistic import Textatistic


final_data['char_count_text'] = final_data['text'].apply(len)
final_data['char_count_processed'] = final_data['processed_text'].apply(len)

# Function that returns number of words in a string
def count_words(string):
    # split the string into words
    words = string.split()

    # return the number of words
    return len(words)

# create a new feature word_count
final_data['word_count_text'] = final_data['text'].apply(count_words)
final_data['word_count_processed'] = final_data['processed_text'].apply(count_words)

#text length
# Compare average character count
average_char_count_text = final_data['char_count_text'].mean()
average_char_count_processed = final_data['char_count_processed'].mean()

# Compare average word count
average_word_count_text = final_data['word_count_text'].mean()
average_word_count_processed = final_data['word_count_processed'].mean()

# Print the results
print("Average Character Count - Text: %.2f" % average_char_count_text)
print("Average Character Count - Processed Text: %.2f" % average_char_count_processed)
print("Average Word Count - Text: %.2f" % average_word_count_text)
print("Average Word Count - Processed Text: %.2f" % average_word_count_processed)

# Calculate average word length
average_word_length_text = final_data['char_count_text'].sum() / final_data['word_count_text'].sum()
average_word_length_processed = final_data['char_count_processed'].sum() / final_data['word_count_processed'].sum()

# Print the results
print("Average Word Length - Text: %.2f" % average_word_length_text)
print("Average Word Length - Processed Text: %.2f" % average_word_length_processed)

"""After processing, the tweets have a smaller average  implying less tokens which may slow down the models"""

#Text readibility score
from textstat import automated_readability_index

# Define a function to calculate ARI
def calculate_ari(text):
    return automated_readability_index(text)

# Apply the function to each text column
ari_text = final_data['text'].apply(calculate_ari)
ari_processed = final_data['processed_text'].apply(calculate_ari)

# Print the results
print("Average ARI - Text: %.2f" % ari_text.mean())
print("Average ARI - Processed Text: %.2f" % ari_processed.mean())

"""The processed text have a better average readability implying it contains more clear sequence of English words and thus, it will be easier to for the processed text to be understood by the model."""

from sklearn.feature_extraction.text import CountVectorizer

# Function to get top N keywords
def get_top_keywords(text_column, n=10):
    vectorizer = CountVectorizer().fit(text_column)
    bag_of_words = vectorizer.transform(text_column)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:n]

# Get top keywords for each text column
top_keywords_text = get_top_keywords(final_data['text'])
top_keywords_processed = get_top_keywords(final_data['processed_text'])

# Print the results
print("Top Keywords - Text:", [keyword[0] for keyword in top_keywords_text])
print("Top Keywords - Processed Text:", [keyword[0] for keyword in top_keywords_processed])

from textblob import TextBlob

# Function to get sentiment
def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

# Get sentiment for each text column
sentiment_text = final_data['text'].apply(get_sentiment)
sentiment_processed = final_data['processed_text'].apply(get_sentiment)

# Print the results
print("Average Sentiment - Text: %.2f" % sentiment_text.mean())
print("Average Sentiment - Processed Text: %.2f" % sentiment_processed.mean())

""""Average Sentiment - Text" and "Average Sentiment - Processed Text" have a value of 0.03. This suggests a slightly positive sentiment. Since the values are close to zero, the sentiment can be considered mildly positive.

**Summary of feature engineering**
Thus, it can be seen that the preprocessed text have a more profound and clear link to the crisis in Ukraine and thus, can be used to make better maping as per accurate inforamtion that the original text.

##Categorising the data using the Openai API
"""

import openai

openai.api_key  = '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'

"""##Prompt engineering and checking best parameters"""

def standard_get_completion(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message["content"]

# Array of tweets
tweets = [
    "The recent clashes in Ukraine have led to a tragic loss of lives. #WarInUkraine",
    "Homes and infrastructure are being destroyed in the conflict zone. #UkraineConflict",
    "Sending thoughts and prayers to the people affected by the war in Ukraine. #StandWithUkraine",
    "Reports of casualties are rising as the conflict intensifies. #WarZone",
    "The international community needs to come together to find a peaceful solution to the Ukraine crisis.",
    "Many families are displaced due to the ongoing war. #Refugees #UkraineWar",
    "The destruction of cultural heritage sites is a heartbreaking consequence of the war.",
    "Providing support to the humanitarian efforts in Ukraine. #HelpUkraine",
    "Today's weather is nice",
    "Stay safe, everyone. Sending love to the people of Ukraine. #PeaceForUkraine"
]

# Corresponding categories for each tweet
categories = [
    " Casualty",
    "Property Damage",
    "support",
    " Casualty",
    "support",
    " Casualty",
    "Property_Damage",
    "support",
    "Irrelevant",
    "support"
]

prompt = f"""Determine whether each of the tweets provided fall into one of the categories:

    Overview of the categories
    Casualty: someone who is injured, killed, captured, or missing in action through engagement with an enemy
    Property Damage: Damage to infrastructures such as health, communications, or transportation can create conditions of further vulnerability for people
    Support: Call for support, sympathy, and companionship
    Irrelevant: Not linked to the war in Ukraine or any crisis situation

    list of tweets: {",".join(tweets)}
    Take less than 2 minutes to categorize the tweet. Output only the category.
    """
response = standard_get_completion(prompt)
print(response)

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Select one tweet at a time and generate a prompt
for i, tweet in enumerate(tweets):
    prompt = generate_prompt(tweet)
    response = standard_get_completion(prompt)
    print(response)

def generate_prompt(tweet):
    prompt = f"""classify the following tweet appropriately based on the context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt
for i, tweet in enumerate(tweets):
    prompt = generate_prompt(tweet)
    response = standard_get_completion(prompt)
    print(response)

def generate_prompt(tweet):
    prompt = f"""determine the category to which the following tweet belong
    Categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt
# Select one tweet at a time and generate a prompt
for i, tweet in enumerate(tweets):
    prompt = generate_prompt(tweet)
    response = standard_get_completion(prompt)
    print(response)

"""Thus, based on the iterations, the prompt with the most accurate preditcion(9/10), implying a 90 percentile accuracy is: **classify the following tweet appropriately based on the context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}**

####Checking for best params
"""

def standard_get_completion_2(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0.5, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message["content"]

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Select one tweet at a time and generate a prompt
for i, tweet in enumerate(tweets):
    prompt = standard_get_completion_2(tweet)
    response = standard_get_completion(prompt)
    print(response)

def standard_get_completion_3(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0.2, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message["content"]

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Select one tweet at a time and generate a prompt
for i, tweet in enumerate(tweets):
    prompt = standard_get_completion_3(tweet)
    response = standard_get_completion(prompt)
    print(response)

"""As model parameter in out model we only have temperature (which is from 0 - 1) and represent the randomness of our model output.

Overall, Increasing the temperature will increase the wordiness in the output, implying a temperature of 0 will be perfect to simply categorize the data based on the inputs without further providing more data.

###Categorising the elements in the dataframe
"""

def get_completion_1(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message["content"]

"""Due to the API limitation of the GPT model at the time of implementation of this task, we need to slice the input into array of 200 elements each to avoid runtime, timeout, API or Server error which have been encountered extensively. After several testing, we acknoledge that the maximum number of input per this cycle of API request is 215 but we prefer selecting 200 for standardisation and conviniency.

From this point, we will only use the first 1000 tweets in out dataset for develoing catgories with the GPT model
"""

data_array_1 = final_data['processed_text'].to_numpy()[:200]
data_array_2 = final_data['processed_text'].to_numpy()[201:400]
data_array_3 = final_data['processed_text'].to_numpy()[401:600]
data_array_4 = final_data['processed_text'].to_numpy()[601:800]
data_array_5 = final_data['processed_text'].to_numpy()[801:1000]

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_1 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_1):
    prompt = generate_prompt(tweet)
    response = get_completion_1(prompt)
    responses_array_1.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_1(prompt)
      responses_array_1.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_1)

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_2 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_2):
    prompt = generate_prompt(tweet)
    response = get_completion_1(prompt)
    responses_array_2.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_1(prompt)
      responses_array_2.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_2)

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_3 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_3):
    prompt = generate_prompt(tweet)
    response = get_completion_1(prompt)
    responses_array_3.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_1(prompt)
      responses_array_3.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_3)

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_4 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_4):
    prompt = generate_prompt(tweet)
    response = get_completion_1(prompt)
    responses_array_4.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_1(prompt)
      responses_array_4.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_4)

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_5 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_5):
    prompt = generate_prompt(tweet)
    response = get_completion_1(prompt)
    responses_array_5.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_1(prompt)
      responses_array_5.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_5)

"""Creating alternative model for model evaluation and comparism"""

def get_completion_2(prompt, model="gpt-4"):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message["content"]

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_11 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_1):
    prompt = generate_prompt(tweet)
    response = get_completion_2(prompt)
    responses_array_11.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_1(prompt)
      responses_array_11.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_11)

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_12 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_2):
    prompt = generate_prompt(tweet)
    response = get_completion_2(prompt)
    responses_array_12.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_2(prompt)
      responses_array_12.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_12)

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_13 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_3):
    prompt = generate_prompt(tweet)
    response = get_completion_2(prompt)
    responses_array_13.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_2(prompt)
      responses_array_13.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_13)

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_14 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_4):
    prompt = generate_prompt(tweet)
    response = get_completion_2(prompt)
    responses_array_14.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_2(prompt)
      responses_array_14.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_14)

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_15 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_5):
    prompt = generate_prompt(tweet)
    response = get_completion_2(prompt)
    responses_array_15.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_2(prompt)
      responses_array_15.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_15)

"""##Joining back the model to the 1000 first elements of the dataframe"""

#combining the arrays of text generated from the GPT 3.5 turbo iterations to create a new data frame
combined_array = np.concatenate((responses_array_1, responses_array_2, responses_array_3, responses_array_4, responses_array_5))

new_data_1 = final_data.head(1000)

new_data_1['categories'] = combined_array[:len(new_data_1)]

new_data_1

#combining the arrays of text generated from the GPT 4 iterations to create a new data frame
combined_array_2 = np.concatenate((responses_array_11, responses_array_12, responses_array_13, responses_array_14, responses_array_15))

new_data_2 = new_data_1.copy()

new_data_2['categories'] = combined_array[:len(new_data_2)]

"""##Model Evaluation"""

#Hamming Distance
hamming_distance = sum(x != y for x, y in zip(new_data_1['categories'], new_data_2['categories']))
hamming_similarity = 1 - (hamming_distance / len(final_data))

print("Hamming Similarity:", hamming_similarity)

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Assuming new_data_1 and new_data_2 are your DataFrames
new_data_1 = pd.DataFrame({
    'categories': ['Support', 'Casualty', 'Irrelevant', 'Property Damage'],
})

new_data_2 = pd.DataFrame({
    'categories': ['Casualty', 'Property Damage', 'Support', 'Irrelevant'],
})

# Combine categories from both DataFrames into a single column
corpus_1 = new_data_1['categories'].astype(str)
corpus_2 = new_data_2['categories'].astype(str)

# Concatenate the two columns
corpus = corpus_1 + ' ' + corpus_2

# Vectorize the text
vectorizer = CountVectorizer().fit_transform(corpus)
cosine_sim = cosine_similarity(vectorizer)

print("Cosine Similarity:")
print(cosine_sim)


# Jaccard Similarity
set1 = set(new_data_1['categories'])
set2 = set(new_data_2['categories'])

intersection = len(set1.intersection(set2))
union = len(set1.union(set2))

jaccard_similarity = intersection / union
print("Jaccard Similarity:", jaccard_similarity)

"""Cosine Similarity:
* Cosine similarity is a measure of similarity between two non-zero vectors in an inner product space that measures the cosine of the angle between them.
* Values range from -1 (completely dissimilar) to 1 (completely similar). A value of 0 indicates orthogonality (no similarity).
* In the output, the matrix shows the cosine similarity between pairs of vectors (categories). For example, the cosine similarity between the first and second categories is approximately 0.408.

Jaccard Similarity:
* Jaccard similarity measures the similarity between two sets by comparing the size of the intersection to the size of the union.
* Values range from 0 (completely dissimilar) to 1 (completely similar). A value of 1 indicates that the sets are identical.
* In the output, the Jaccard similarity is 1.0, indicating that the sets of categories are identical.

Hamming Similarity:
* Hamming similarity measures the similarity between two strings of equal length by counting the number of positions at which the corresponding symbols are the same.
* Values range from 0 (completely dissimilar) to 1 (completely similar). A value of 1 indicates that the strings are identical.
* In the output, the Hamming similarity is 1.0, indicating that the sets of categories are identical.

###Adding all 1000 element categorised using the GPT 4 model to the initial 1000
"""

data_array_1 = final_data['processed_text'].to_numpy()[1000:2000]

def generate_prompt(tweet):
    prompt = f"""classify its context into one of the categories: Casualty, Property Damage, support and Irrelevant.{tweet}"""
    return prompt

# Example usage:
responses_array_16 = []  # Initialize an array to store responses
counter = 0

for i, tweet in enumerate(data_array_1):
    prompt = generate_prompt(tweet)
    response = get_completion_2(prompt)
    responses_array_16.append(response)
    print(response)

    # Increment the counter
    counter += 1

    # Check if it's time to make a new API request (every 20 tweets)
    if counter == 20:
      reponse=get_completion_2(prompt)
      responses_array_16.append(response)
      print(response)
        # Reset the counter
      counter = 0

print("Responses Array:", responses_array_16)

#combining the arrays of text generated from the GPT 4 turbo iterations to create a new data frame
combined_array_3 = np.concatenate((responses_array_11, responses_array_12, responses_array_13, responses_array_14, responses_array_15,responses_array_16 ))

new_data_3 = final_data.head(2000)

new_data_3['categories'] = combined_array_3[:len(new_data_3)]

"""## Filtering the essential categories and Saving the dataset

Getting more data with limited GPT 4 token after testing the accuracy of both models
"""

allowed_categories = ['Support', 'Casualty', 'Irrelevant', 'Property Damage']

# Filter the DataFrame based on allowed categories
filtered_data = new_data_3[new_data_3['categories'].isin(allowed_categories)]

# Print the filtered DataFrame
filtered_data

# Assuming your DataFrame is named final_data and you want to save it to a file named 'final_data_with_categories.csv'
filtered_data.to_csv('final_data_with_categories.csv', index=False)